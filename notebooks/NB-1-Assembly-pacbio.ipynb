{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembly and analysis of Ficus RAD-seq data\n",
    "\n",
    "See notebook 1 for demultiplexing raw reads to separate files for each sample. This notebook assembles several datasets from the 180 samples contained in the two libraries sequenced across 4 lanes of Illumina HiSeq 3000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install ipyrad -c ipyrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipyrad v.0.9.50\n"
     ]
    }
   ],
   "source": [
    "import ipyrad as ip\n",
    "import pandas as pd\n",
    "import os\n",
    "print('ipyrad v.{}'.format(ip.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The reference genome\n",
    "A reference *Ficus carica* genome was is available at at [NCBI](https://www.ncbi.nlm.nih.gov/genome/?term=common+fig). It can be downloaded by executing the cell below. If the file is already present in the current directory it will skip downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading: GCA_009761775.1_UNIPI_FiCari_1.0_genomic.fna.gz\n",
      "--2020-04-08 17:46:22--  ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/009/761/775/GCA_009761775.1_UNIPI_FiCari_1.0/GCA_009761775.1_UNIPI_FiCari_1.0_genomic.fna.gz\n",
      "           => ‘GCA_009761775.1_UNIPI_FiCari_1.0_genomic.fna.gz’\n",
      "Resolving ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)... 130.14.250.12\n",
      "Connecting to ftp.ncbi.nlm.nih.gov (ftp.ncbi.nlm.nih.gov)|130.14.250.12|:21... connected.\n",
      "Logging in as anonymous ... Logged in!\n",
      "==> SYST ... done.    ==> PWD ... done.\n",
      "==> TYPE I ... done.  ==> CWD (1) /genomes/all/GCA/009/761/775/GCA_009761775.1_UNIPI_FiCari_1.0 ... done.\n",
      "==> SIZE GCA_009761775.1_UNIPI_FiCari_1.0_genomic.fna.gz ... 103523264\n",
      "==> PASV ... done.    ==> RETR GCA_009761775.1_UNIPI_FiCari_1.0_genomic.fna.gz ... done.\n",
      "Length: 103523264 (99M) (unauthoritative)\n",
      "\n",
      "GCA_009761775.1_UNI 100%[===================>]  98.73M  27.8MB/s    in 3.6s    \n",
      "\n",
      "2020-04-08 17:46:26 (27.8 MB/s) - ‘GCA_009761775.1_UNIPI_FiCari_1.0_genomic.fna.gz’ saved [103523264]\n",
      "\n",
      "decompressing: GCA_009761775.1_UNIPI_FiCari_1.0_genomic.fna.gz\n",
      "fasta file: GCA_009761775.1_UNIPI_FiCari_1.0_genomic.fna\n"
     ]
    }
   ],
   "source": [
    "# The reference genome link\n",
    "reference = (\n",
    "    \"ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/\"\n",
    "    \"009/761/775/GCA_009761775.1_UNIPI_FiCari_1.0/\"\n",
    "    \"GCA_009761775.1_UNIPI_FiCari_1.0_genomic.fna.gz\"\n",
    ")\n",
    "\n",
    "# get the file name\n",
    "gzip_fname = reference.split(\"/\")[-1]\n",
    "fname = gzip_fname[:-3]\n",
    "\n",
    "# download and decompress file and name it {name}.fna\n",
    "if os.path.exists(gzip_fname):\n",
    "    ! gunzip ./GCA_009761775.1_UNIPI_FiCari_1.0_genomic.fna.gz\n",
    "    print('decompressing: {}'.format(gzip_fname))\n",
    "\n",
    "if os.path.exists(fname):\n",
    "    print('fasta file: {}'.format(fname))\n",
    "else:\n",
    "    ## Download the reference genome of F. carica\n",
    "    print('downloading: {}'.format(gzip_fname))\n",
    "    ! wget $reference\n",
    "    print('decompressing: {}'.format(gzip_fname))\n",
    "    ! gunzip ./GCA_009761775.1_UNIPI_FiCari_1.0_genomic.fna.gz\n",
    "    print('fasta file: {}'.format(fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Demultiplexed fastq files \n",
    "\n",
    "See notebook1 for details of demultiplexing. Here we load the four lanes of data as sorted fastq data since this is the way that others would download the data from NCBI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup assembly object to load the four libraries\n",
    "demux1 = ip.Assembly(\"demux1\")\n",
    "demux1.params.project_dir = \"./analysis-ipyrad\"\n",
    "demux1.params.sorted_fastq_path = \"./demux_fastqs/lib1lane1_fastqs/*.gz\"\n",
    "\n",
    "demux2 = ip.Assembly(\"demux2\")\n",
    "demux2.params.project_dir = \"./analysis-ipyrad\"\n",
    "demux2.params.sorted_fastq_path = \"./demux_fastqs/lib1lane2_fastqs/*.gz\"\n",
    "\n",
    "demux3 = ip.Assembly(\"demux3\")\n",
    "demux3.params.project_dir = \"./analysis-ipyrad\"\n",
    "demux3.params.sorted_fastq_path = \"./demux_fastqs/lib2lane1_fastqs/*.gz\"\n",
    "\n",
    "demux4 = ip.Assembly(\"demux4\")\n",
    "demux4.params.project_dir = \"./analysis-ipyrad\"\n",
    "demux4.params.sorted_fastq_path = \"./demux_fastqs/lib2lane2_fastqs/*.gz\"\n",
    "\n",
    "# load fastq files for the four lanes of data\n",
    "demux1.run(\"1\", auto=True, quiet=True, force=True)\n",
    "demux2.run(\"1\", auto=True, quiet=True, force=True)\n",
    "demux3.run(\"1\", auto=True, quiet=True, force=True)\n",
    "demux4.run(\"1\", auto=True, quiet=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge assemblies into a single one\n",
    "data = ip.merge(\"allmerged\", [demux1, demux2, demux3, demux4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples that we may want to exclude later\n",
    "data.stats[data.stats.reads_raw < 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop low coverage samples and the CONTROL \n",
    "keep = [i for i in data.samples if i != \"FGXCONTROL\"]\n",
    "data = data.branch(\"ficus-190\", keep, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters for assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set several non-default parameters\n",
    "data.params.project_dir = \"analysis-ipyrad\"\n",
    "data.params.assembly_method = \"reference\"\n",
    "data.params.reference_sequence = \"./GCA_009761775.1_UNIPI_FiCari_1.0_genomic.fna\"\n",
    "data.params.filter_adapters = 3\n",
    "data.params.phred_Qscore_offset = 43\n",
    "data.params.max_Hs_consens = 0.1\n",
    "data.params.max_shared_Hs_locus = 0.1\n",
    "data.params.filter_min_trim_len = 50\n",
    "data.params.output_formats = \"lp\"\n",
    "\n",
    "# print parameters for prosperity's sake\n",
    "data.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A summary of the number of reads per Sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary of raw read covereage\n",
      "count         190\n",
      "mean      6039582\n",
      "std       8967873\n",
      "min          5463\n",
      "25%        250209\n",
      "50%       2013696\n",
      "75%       8209965\n",
      "max      51911404\n",
      "Name: reads_raw, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"summary of raw read covereage\")\n",
    "print(data.stats.reads_raw.describe().astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering options explained\n",
    "\n",
    "From looking closely at the data it appears there are some poor quality reads with adapter contamination, and also that there are some conspicuous long strings of poly repeats, which are probably due to the library being put on the sequencer in the wrong concentration (the facility failed to do a qPCR quantification). Setting the filter parameter in ipyrad to strict (2) uses 'cutadapt' to filter the reads. By default ipyrad would look just for the Illumina universal adapter, but I'm also adding a few additional poly-{A,C,G,T} sequences to be trimmed. These appeared to be somewhat common in the raw data, followed by nonsense.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Within-sample assembly\n",
    "\n",
    "Steps 2-5 of ipyrad function to filter and cluster reads, and to call consensus haplotypes within samples. We'll look more closely at the stats for each step after it's finished.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Assembly: ficus-190\n",
      "from saved path: ~/Documents/Ficus/analysis-ipyrad/ficus-190.json\n"
     ]
    }
   ],
   "source": [
    "data = ip.load_json(\"./analysis-ipyrad/ficus-190.json\")\n",
    "pbdata = data.branch(\"ficus-190-pb\")\n",
    "pbdata.params.reference_sequence = \"./GCA_009761775.1_UNIPI_FiCari_1.0_genomic.fna\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel connection | 1efffc7603ac: 28 cores\n",
      "[####################] 100% 0:05:21 | indexing reference   | s3 |\n",
      "[####################] 100% 0:09:30 | dereplicating        | s3 |\n",
      "[####################] 100% 0:15:51 | mapping reads        | s3 |\n",
      "[####################] 100% 0:09:44 | building clusters    | s3 |\n",
      "[####################] 100% 0:00:34 | calc cluster stats   | s3 |\n",
      "[####################] 100% 0:09:51 | inferring [H, E]     | s4 |\n",
      "[####################] 100% 0:00:27 | calculating depths   | s5 |\n",
      "[####################] 100% 0:00:32 | chunking clusters    | s5 |\n",
      "[####################] 100% 2:35:15 | consens calling      | s5 |\n",
      "[####################] 100% 0:01:42 | indexing alleles     | s5 |\n"
     ]
    }
   ],
   "source": [
    "pbdata.ipcluster[\"cores\"] = 28\n",
    "pbdata.run(\"345\", auto=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A29_popenoei              570\n",
       "A62_turbinata             812\n",
       "A63_turbinata             533\n",
       "C02_citrifolia             23\n",
       "C09_costaricana            75\n",
       "C32_triangleXtrigonata    694\n",
       "C33_triangle              685\n",
       "C34_triangle              129\n",
       "C52_citrifolia            108\n",
       "Fbul-2                    117\n",
       "Fcol-1                    199\n",
       "Fcos-3                    421\n",
       "Fcot-3                    207\n",
       "Fgom-2                    481\n",
       "Fobt-1                    787\n",
       "Fobt-5                    732\n",
       "Fpop-1                    396\n",
       "Fpop-2                    395\n",
       "Ftria-1                   175\n",
       "Name: reads_consens, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all samples with <5000 consensus reads\n",
    "drop = pbdata.stats[pbdata.stats.reads_consens < 1000]\n",
    "drop.reads_consens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all samples with <5000 consensus reads\n",
    "keep = pbdata.stats[pbdata.stats.reads_consens >= 1000].index.tolist()\n",
    "\n",
    "# create branch with only hidepth samples\n",
    "subdata = pbdata.branch(\"ficus-190-pb-1Kmin\", subsamples=keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel connection | 1efffc7603ac: 28 cores\n",
      "[####################] 100% 0:00:39 | concatenating bams   | s6 |\n",
      "[####################] 100% 0:00:19 | fetching regions     | s6 |\n",
      "[####################] 100% 0:00:54 | building database    | s6 |\n"
     ]
    }
   ],
   "source": [
    "subdata.run('6', auto=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel connection | 1efffc7603ac: 28 cores\n",
      "[####################] 100% 0:01:00 | applying filters     | s7 |\n",
      "[####################] 100% 0:15:38 | building arrays      | s7 |\n",
      "[####################] 100% 0:05:43 | writing conversions  | s7 |\n"
     ]
    }
   ],
   "source": [
    "ficus_min4 = subdata.branch(\"ficus-190-pb-min4\")\n",
    "ficus_min4.params.min_samples_locus = 4\n",
    "ficus_min4.params.max_SNPs_locus = 0.25\n",
    "ficus_min4.params.output_formats = \"pslu\"\n",
    "ficus_min4.run(\"7\", auto=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
